{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: tty8128\n",
    "# Time: 2019-11-3\n",
    "\n",
    "# tokenize: small sentence\n",
    "# PunktSentenceTokenizer: large sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\" Welcome readers. I hope you find it interesting. Please do reply.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to cut large batches of sentences, use PunktSentenceTokenizer\n",
    "# for different languages, change the pickle\n",
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Hello everyone. Hope all are fine and doing well. Hope you find the book interesting.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.',\n",
       " 'Hope all are fine and doing well.',\n",
       " 'Hope you find the book interesting.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for single sentence, use this to cut it into words\n",
    "text=nltk.word_tokenize('PierreVinken, 59 years old, will join as a nonexecutive director on Nov. 29 .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PierreVinken', ',', '59', 'years', 'old', ',', 'will', 'join', 'as', 'a', 'nonexecutive', 'director', 'on', 'Nov.', '29', '.']\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please write a text1\n"
     ]
    }
   ],
   "source": [
    "r=input(\"Please write a text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of text is 1 words\n"
     ]
    }
   ],
   "source": [
    "print (\"The length of text is\",len(word_tokenize(r)),\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TreebankWordTokenizer is depend on Penn Treebank rule, while PunktWordTokenizer use the symbols\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions', '.'] ['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions', '.']\n"
     ]
    }
   ],
   "source": [
    "text=\"Don't hesitate to ask questions.\"\n",
    "treeTokenizer=TreebankWordTokenizer()\n",
    "punckTokenizer=WordPunctTokenizer()\n",
    "ans1=treeTokenizer.tokenize(text)\n",
    "ans2=punckTokenizer.tokenize(text)\n",
    "print(ans1,ans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Don't\", 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using reg for tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "regTokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "regTokenizer.tokenize(\"Don't hesitate to ask questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'t\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "# if you don't want to initial a instance\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "sent=\"Don't hesitate to ask questions\"\n",
    "print(regexp_tokenize(sent, pattern='\\w+|\\$[\\d\\.]+|\\S+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer('\\s+',gaps=True)\n",
    "print(tokenizer.tokenize(\"Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'She']\n"
     ]
    }
   ],
   "source": [
    "sent=\" She secured 90.56 % in class X . She is a meritorious student\"\n",
    "capt = RegexpTokenizer('[A-Z]\\w+')\n",
    "print(capt.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' She secured 90.56 % in class X . She is a meritorious student']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent=\" She secured 90.56 % in class X . She is a meritorious student\"\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "BlanklineTokenizer().tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'secured', '90.56', '%', 'in', 'class', 'X', '.', 'She', 'is', 'a', 'meritorious', 'student']\n"
     ]
    }
   ],
   "source": [
    "sent=\" She secured 90.56 % in class X . She is a meritorious student\"\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "print(WhitespaceTokenizer().tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'secured', '90.56', '%', 'in', 'class', 'X.', 'She', 'is', 'a', 'meritorious', 'student']\n",
      "['', 'She', 'secured', '90.56', '%', 'in', 'class', 'X.', 'She', 'is', 'a', 'meritorious', 'student']\n",
      "[' She secured 90.56 % in class X ', '. She is a meritorious student', '']\n"
     ]
    }
   ],
   "source": [
    "sent= \" She secured 90.56 % in class X. She is a meritorious student\"\n",
    "print(sent.split())\n",
    "print(sent.split(' '))\n",
    "sent=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "print(sent.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' She secured 90.56 % in class X \\n. She is a meritorious student\\n']\n",
      "[' She secured 90.56 % in class X ', '. She is a meritorious student']\n",
      "[' She secured 90.56 % in class X ', '. She is a meritorious student']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer\n",
    "sent=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "print(BlanklineTokenizer().tokenize(sent))\n",
    "from nltk.tokenize import LineTokenizer\n",
    "print(LineTokenizer(blanklines='keep').tokenize(sent))\n",
    "print(LineTokenizer(blanklines='discard').tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'She', 'secured', '90.56', '%', 'in', 'class', 'X', '\\n.', 'She', 'is', 'a', 'meritorious', 'student\\n']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import SpaceTokenizer\n",
    "print(SpaceTokenizer().tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (5, 12), (13, 18), (19, 20), (21, 23), (24, 29), (30, 31), (33, 34), (35, 38), (39, 41), (42, 43), (44, 55), (56, 63)]\n"
     ]
    }
   ],
   "source": [
    "# return the offset of each word\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "sent=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "print(list(WhitespaceTokenizer().span_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (1, 7), (1, 5), (1, 1), (1, 2), (1, 5), (1, 1), (2, 1), (1, 3), (1, 2), (1, 1), (1, 11), (1, 7)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize.util import spans_to_relative\n",
    "sent=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "print(list(spans_to_relative(WhitespaceTokenizer().span_tokenize(sent))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (5, 12), (13, 18), (19, 20), (21, 23), (24, 29), (30, 31), (32, 34), (35, 38), (39, 41), (42, 43), (44, 55), (56, 64)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.util import string_span_tokenize\n",
    "sent=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "print(list(string_span_tokenize(sent, \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
